{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5e52a-4ba4-421f-9012-8246ccf8a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e48a47-943b-4bf8-8dfd-56322a33e07b",
   "metadata": {},
   "source": [
    "## Read CSV file and print Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39ea448-f244-4f0f-9d55-e386b81a034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1435c726-8132-493d-9d88-bcad13423d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 206 entries, 0 to 205\n",
      "Data columns (total 10 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   User_ID                     103 non-null    float64\n",
      " 1   Age                         103 non-null    object \n",
      " 2   Gender                      103 non-null    object \n",
      " 3   Platform                    103 non-null    object \n",
      " 4   Daily_Usage_Time (minutes)  103 non-null    float64\n",
      " 5   Posts_Per_Day               103 non-null    float64\n",
      " 6   Likes_Received_Per_Day      103 non-null    float64\n",
      " 7   Comments_Received_Per_Day   103 non-null    float64\n",
      " 8   Messages_Sent_Per_Day       103 non-null    float64\n",
      " 9   Dominant_Emotion            103 non-null    object \n",
      "dtypes: float64(6), object(4)\n",
      "memory usage: 16.2+ KB\n",
      "None\n",
      "\n",
      "First few rows of the dataframe:\n",
      "   User_ID  Age      Gender  Platform  Daily_Usage_Time (minutes)  \\\n",
      "0      NaN  NaN         NaN       NaN                         NaN   \n",
      "1    500.0   27      Female  Snapchat                       120.0   \n",
      "2      NaN  NaN         NaN       NaN                         NaN   \n",
      "3    488.0   21  Non-binary  Snapchat                        60.0   \n",
      "4      NaN  NaN         NaN       NaN                         NaN   \n",
      "\n",
      "   Posts_Per_Day  Likes_Received_Per_Day  Comments_Received_Per_Day  \\\n",
      "0            NaN                     NaN                        NaN   \n",
      "1            4.0                    40.0                       18.0   \n",
      "2            NaN                     NaN                        NaN   \n",
      "3            1.0                    18.0                        7.0   \n",
      "4            NaN                     NaN                        NaN   \n",
      "\n",
      "   Messages_Sent_Per_Day Dominant_Emotion  \n",
      "0                    NaN              NaN  \n",
      "1                   22.0          Neutral  \n",
      "2                    NaN              NaN  \n",
      "3                   12.0          Neutral  \n",
      "4                    NaN              NaN  \n"
     ]
    }
   ],
   "source": [
    "# Display basic information and identify issues\n",
    "print(\"Dataframe Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows of the dataframe:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a2409-b4a9-40d0-aa44-16920f749939",
   "metadata": {},
   "source": [
    "## Data Cleaning Script for Social Media User Data \n",
    "\n",
    "# This script cleans a CSV file containing social media user data. It performs the following tasks:\n",
    "\n",
    "Load Data: Imports the pandas library and reads the CSV file into a DataFrame named df.\n",
    "Data Exploration:\n",
    "Prints basic information about the DataFrame using .info().\n",
    "Prints the first few rows using .head() to identify potential issues.\n",
    "# Missing Values:\n",
    "Prints the number of missing values per column using .isnull().sum().\n",
    "Drops rows with any missing values using .dropna() and stores the cleaned data in df_cleaned.\n",
    "Data Type Conversion:\n",
    "Iterates through specific columns and attempts to convert their values to numeric data types using .to_numeric().\n",
    "Uses the errors='coerce' argument to handle non-numeric values by converting them to NaN (Not a Number).\n",
    "Updates the data types in-place using .loc for better memory efficiency.\n",
    "Drops rows with conversion errors using .dropna().\n",
    "# Duplicate Removal:\n",
    "Removes duplicate rows from df_cleaned using .drop_duplicates().\n",
    "# Outlier Detection and Removal:\n",
    "Imports the zscore function from scipy.stats for z-score calculation.\n",
    "Defines a function remove_outliers that calculates z-scores for a specified column.\n",
    "# Filters the DataFrame to keep rows with z-scores within 3 standard deviations of the mean, effectively removing outliers.\n",
    "Drops the temporary zscore column after filtering.\n",
    "Applies the remove_outliers function to relevant columns containing numeric data (e.g., Daily_Usage_Time, Posts_Per_Day).\n",
    "# Saving Cleaned Data:\n",
    "Saves the cleaned DataFrame df_cleaned to a new CSV file named cleaned_file.csv using .to_csv().\n",
    "Sets index=False to avoid saving the row index in the output file.\n",
    "Confirmation:\n",
    "Prints basic information about the cleaned DataFrame using .info() for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5e901-6446-4e5a-8fb7-d6b6eec1ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('./test.csv')\n",
    "\n",
    "# Display basic information and identify issues\n",
    "print(\"Dataframe Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows of the dataframe:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Convert columns to appropriate data types using .loc\n",
    "df_cleaned.loc[:, 'Age'] = pd.to_numeric(df_cleaned['Age'], errors='coerce')\n",
    "df_cleaned.loc[:, 'Daily_Usage_Time (minutes)'] = pd.to_numeric(df_cleaned['Daily_Usage_Time (minutes)'], errors='coerce')\n",
    "df_cleaned.loc[:, 'Posts_Per_Day'] = pd.to_numeric(df_cleaned['Posts_Per_Day'], errors='coerce')\n",
    "df_cleaned.loc[:, 'Likes_Received_Per_Day'] = pd.to_numeric(df_cleaned['Likes_Received_Per_Day'], errors='coerce')\n",
    "df_cleaned.loc[:, 'Comments_Received_Per_Day'] = pd.to_numeric(df_cleaned['Comments_Received_Per_Day'], errors='coerce')\n",
    "df_cleaned.loc[:, 'Messages_Sent_Per_Day'] = pd.to_numeric(df_cleaned['Messages_Sent_Per_Day'], errors='coerce')\n",
    "\n",
    "# Drop rows with conversion errors\n",
    "df_cleaned = df_cleaned.dropna()\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "\n",
    "# Define a function to remove outliers based on z-score\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def remove_outliers(df, column):\n",
    "    df['zscore'] = zscore(df[column])\n",
    "    df = df[abs(df['zscore']) < 3]\n",
    "    df = df.drop(columns=['zscore'])\n",
    "    return df\n",
    "\n",
    "# Apply the function to relevant columns\n",
    "df_cleaned = remove_outliers(df_cleaned, 'Daily_Usage_Time (minutes)')\n",
    "df_cleaned = remove_outliers(df_cleaned, 'Posts_Per_Day')\n",
    "df_cleaned = remove_outliers(df_cleaned, 'Likes_Received_Per_Day')\n",
    "df_cleaned = remove_outliers(df_cleaned, 'Comments_Received_Per_Day')\n",
    "df_cleaned = remove_outliers(df_cleaned, 'Messages_Sent_Per_Day')\n",
    "\n",
    "# Save the cleaned dataframe to a new CSV file\n",
    "df_cleaned.to_csv('cleaned_file.csv', index=False)\n",
    "\n",
    "print(\"\\nDataframe after cleaning and saving:\")\n",
    "print(df_cleaned.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57dc02-f44a-457a-b26a-7ff8b0ab3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mrjob pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f649214-52c8-4d52-a4e9-94987e8c66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mrjob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d29c2-64c5-4b6e-bf2f-1fb4834bcd81",
   "metadata": {},
   "source": [
    "# **Filtering Data:**\n",
    "\n",
    "## Filter rows based on specific conditions using the filter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d39ee-d371-4fd9-ba67-342b1578d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filtered_df.filter(data_df.Age > 25)  # Select rows where Age is greater than 25\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6437d261-bdec-4d6f-8431-d8d447c750f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select specified column and grouping and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102bf750-a2d4-4fad-9fb8-aa5a13b1cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "selected_data = filtered_data.select(\"User_ID\",\"Daily_Usage_Time (minutes)\",\"Likes_Received_Per_Day\")\n",
    "print(\"Selected Columns User_ID,Daily_Usage_Time (minutes), Likes_Received_Per_Day\")\n",
    "selected_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff3f0e-6538-46f7-a9a6-0a482febefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group By and Aggregation (replace columns as needed)\n",
    "avg_usage_by_platform = filtered_data.groupBy(\"Platform\").agg(mean(\"Daily_Usage_Time (minutes)\").alias(\"Avg_Daily_Usage_Time\"))\n",
    "print(\"Average Daily Usage Time per Platform:\")\n",
    "avg_usage_by_platform.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37040c9-f80a-4b8b-87e1-67b53f59b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistics (replace columns as needed)\n",
    "print(\"Descriptive Statistics (Daily_Usage_Time, Age, Posts_Per_Day):\")\n",
    "data_df.describe(\"Daily_Usage_Time (minutes)\", \"Age\", \"Posts_Per_Day\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b56df-7fbd-475e-a7a2-4f2987c90e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis and UDFs can be included here, showing output after each step\n",
    "\n",
    "# Stop the SparkSession (local environment)\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35df21b-3235-441d-a0e4-09e342bb6c6d",
   "metadata": {},
   "source": [
    "**My input file is input.csv.in WSL Ubuntu 22.4 i run dominant_emotion.py (mapreduce code),below is mapreduce code.**\n",
    "\n",
    "hdfs dfs -put /tmp/input.csv /test_data/input.csv .I run this command in terminal to put it hdfs and run it in hadoop **python3 run_job.py** i store output getting from Hadoop in Hdfs .i download output.csv from HDFS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490d81f-a988-45fc-89eb-77ee2d619043",
   "metadata": {},
   "source": [
    "## TASK 1: Find out emotion count based on gender using Mapreduce in Hadoop framework using HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d32af-d400-45da-bf32-c81c7283d2fa",
   "metadata": {},
   "source": [
    "![Example Image](Dominantemotion.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df42ad3-ece3-419b-8ead-402f30ac9953",
   "metadata": {},
   "source": [
    "![Example Image](Hadoop_imosioncount.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77ff51-6083-4835-ab31-c4cb2e010f8c",
   "metadata": {},
   "source": [
    "![Example Image](hadoop.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f272f-42e6-48ad-83f7-a0df59cb48b6",
   "metadata": {},
   "source": [
    "![Example Image](get.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b0164-a0ac-49f9-b824-99cbd1c54c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class DominantEmotion(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_emotions,\n",
    "                   reducer=self.reducer_count_emotions)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_emotions(self, _, line):\n",
    "        # Skip the header\n",
    "        if line.startswith('User_ID'):\n",
    "            return\n",
    "\n",
    "        parts = line.split(',')\n",
    "\n",
    "        # Ensure there are enough parts in the line to avoid index errors\n",
    "        if len(parts) > 9:\n",
    "            # Extract gender and dominant emotion\n",
    "            gender = parts[2]\n",
    "            emotion = parts[9]\n",
    "\n",
    "            yield gender, emotion\n",
    "\n",
    "    def reducer_count_emotions(self, key, values):\n",
    "        emotion_counts = {}\n",
    "        for emotion in values:\n",
    "            if emotion in emotion_counts:\n",
    "                emotion_counts[emotion] += 1\n",
    "            else:\n",
    "                emotion_counts[emotion] = 1\n",
    "        yield key, emotion_counts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DominantEmotion.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648419a1-bd18-4c74-bed1-04b2a29f0ed5",
   "metadata": {},
   "source": [
    "## Drivercode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ece3e3-8cb7-4b57-bd87-5ec68718d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from dominant_emotion import DominantEmotion\n",
    "\n",
    "def run_mrjob(input_file, output_dir):\n",
    "    # Create a temporary local file to store the intermediate output\n",
    "    temp_output_file = tempfile.NamedTemporaryFile(delete=False, mode='w+')\n",
    "    temp_output_filename = temp_output_file.name\n",
    "\n",
    "    try:\n",
    "        # Remove the existing output directory if it exists\n",
    "        subprocess.run(['hdfs', 'dfs', '-rm', '-r', output_dir], check=False)\n",
    "\n",
    "        # Run the MRJob with HDFS input and temporary local output\n",
    "        mr_job = DominantEmotion(args=['-r', 'hadoop', input_file, '--output-dir', output_dir])\n",
    "\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "            # Read the intermediate output from HDFS\n",
    "            subprocess.run(['hdfs', 'dfs', '-getmerge', f'{output_dir}/part-*', temp_output_filename], check=True)\n",
    "\n",
    "            # Write the final output to the desired HDFS path\n",
    "            final_output_path = f'{output_dir}/output_emotion.csv'\n",
    "            with open(temp_output_filename, 'r') as infile:\n",
    "                with open('final_output.csv', 'w', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow(['Gender', 'Emotion', 'Count'])\n",
    "\n",
    "                    for line in infile:\n",
    "                        key, value = line.strip().split('\\t')\n",
    "                        gender, emotion_counts = key, eval(value)\n",
    "                        for emotion, count in emotion_counts.items():\n",
    "                            writer.writerow([gender, emotion, count])\n",
    "\n",
    "            # Put the final output CSV to HDFS\n",
    "            subprocess.run(['hdfs', 'dfs', '-put', 'final_output.csv', final_output_path], check=True)\n",
    "\n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        os.remove(temp_output_filename)\n",
    "        if os.path.exists('final_output.csv'):\n",
    "            os.remove('final_output.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'hdfs:///test_data/input.csv'  # Your input HDFS CSV file\n",
    "    output_dir = 'hdfs:///output_data/'  # Your output HDFS directory\n",
    "    run_mrjob(input_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c0f88-844f-4f6c-8193-5f4732e3aea7",
   "metadata": {},
   "source": [
    "## TASK 2 To find out count Based on Emotion using Mapreduce\n",
    "Mapper: mapper_extract_emotion\n",
    "The mapper function extracts the dominant emotion from each line of input and yields it along with a count of 1. This function is called once for each line of the input file.\n",
    "Reducer: reducer_count_emotions\n",
    "The reducer function aggregates the counts of each emotion emitted by the mapper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb70302-b34a-47ec-8b31-656b2ef1547d",
   "metadata": {},
   "source": [
    "![Example Image](emptioncount.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7c920-d7f2-4168-ab12-14ffb8c8a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class EmotionCounts(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_extract_emotion,\n",
    "                   reducer=self.reducer_count_emotions)\n",
    "        ]\n",
    "\n",
    "    def mapper_extract_emotion(self, _, line):\n",
    "        # Skip the header\n",
    "        if line.startswith('User_ID'):\n",
    "            return\n",
    "\n",
    "        parts = line.split(',')\n",
    "\n",
    "        # Ensure there are enough parts in the line to avoid index errors\n",
    "        if len(parts) > 9:\n",
    "            # Extract dominant emotion\n",
    "            emotion = parts[9]\n",
    "\n",
    "            yield emotion, 1\n",
    "\n",
    "    def reducer_count_emotions(self, emotion, counts):\n",
    "        yield emotion, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    EmotionCounts.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b2857-5b75-47c0-ba27-312da955d631",
   "metadata": {},
   "source": [
    "# Driver code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7784bb-986a-4238-b6aa-0a76307ea8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from emotion_counts import EmotionCounts\n",
    "\n",
    "def run_mrjob(input_file, output_dir):\n",
    "    # Create a temporary local file to store the intermediate output\n",
    "    temp_output_file = tempfile.NamedTemporaryFile(delete=False, mode='w+')\n",
    "    temp_output_filename = temp_output_file.name\n",
    "\n",
    "    try:\n",
    "        # Remove the existing output directory if it exists\n",
    "        subprocess.run(['hdfs', 'dfs', '-rm', '-r', output_dir], check=False)\n",
    "\n",
    "        # Run the MRJob with HDFS input and temporary local output\n",
    "        mr_job = EmotionCounts(args=['-r', 'hadoop', input_file, '--output-dir', output_dir])\n",
    "\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "            # Read the intermediate output from HDFS\n",
    "            subprocess.run(['hdfs', 'dfs', '-getmerge', f'{output_dir}/part-*', temp_output_filename], check=True)\n",
    "\n",
    "            # Write the final output to the desired HDFS path\n",
    "            final_output_path = f'{output_dir}/emotion_counts.csv'\n",
    "            with open(temp_output_filename, 'r') as infile:\n",
    "                with open('final_output.csv', 'w', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow(['Emotion', 'Count'])\n",
    "\n",
    "                    for line in infile:\n",
    "                        emotion, count = line.strip().split('\\t')\n",
    "                        writer.writerow([emotion, count])\n",
    "\n",
    "            # Put the final output CSV to HDFS\n",
    "            subprocess.run(['hdfs', 'dfs', '-put', 'final_output.csv', final_output_path], check=True)\n",
    "\n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        os.remove(temp_output_filename)\n",
    "        if os.path.exists('final_output.csv'):\n",
    "            os.remove('final_output.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'hdfs:///test_data/input.csv'  # Your input HDFS CSV file\n",
    "    output_dir = 'hdfs:///output_data/'  # Your output HDFS directory\n",
    "    run_mrjob(input_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c5b1a0-aa4b-4dd1-b716-eb52c22370f8",
   "metadata": {},
   "source": [
    "## TASK3 :Average Daily Usage Time by Platform by gender\n",
    "\n",
    "Mapper: Reads each line of the CSV, extracts the platform and daily usage time, and emits a tuple (platform, daily usage time).\n",
    "\n",
    "Reducer: Aggregates the total daily usage time and the count of users per platform, then calculates the average daily usage time for each platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c3b4b5-1519-41cf-8398-1c79e62a8c89",
   "metadata": {},
   "source": [
    "![Example Image](output_gender.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4d8de-3912-49cd-adde-2a6b255ba220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from platform_by_gender import PlatformByGender\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "# Function to run the MapReduce job\n",
    "def run_mrjob(input_file, output_dir):\n",
    "    # Create a temporary local file to store the intermediate output\n",
    "    temp_output_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    temp_output_filename = temp_output_file.name\n",
    "\n",
    "    try:\n",
    "        # Remove the existing output directory if it exists\n",
    "        subprocess.run(['hdfs', 'dfs', '-rm', '-r', output_dir], check=False)\n",
    "\n",
    "        # Run the MRJob with HDFS input and temporary local output\n",
    "        mr_job = PlatformByGender(args=['-r', 'hadoop', input_file, '--output-dir', output_dir])\n",
    "\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "            # Read the intermediate output from HDFS\n",
    "            subprocess.run(['hdfs', 'dfs', '-getmerge', f'{output_dir}/part-*', temp_output_filename], check=True)\n",
    "\n",
    "            # Write the final output to the desired HDFS path\n",
    "            final_output_path = f'{output_dir}/output_gender.csv'\n",
    "            with open(temp_output_filename, 'r') as infile:\n",
    "                with open('final_output.csv', 'w', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow(['Gender', 'Platform', 'Count'])\n",
    "\n",
    "                    # Read each line and split it by tab ('\\t') to separate key and value\n",
    "                    for line in infile:\n",
    "                        key, value = line.strip().split('\\t')\n",
    "                        gender, platform = eval(key)  # Parse key\n",
    "                        count = int(value)  # Convert value to integer\n",
    "                        writer.writerow([gender, platform, count])\n",
    "\n",
    "            # Put the final output CSV to HDFS\n",
    "            subprocess.run(['hdfs', 'dfs', '-put', 'final_output.csv', final_output_path], check=True)\n",
    "\n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        os.remove(temp_output_filename)\n",
    "        if os.path.exists('final_output.csv'):\n",
    "            os.remove('final_output.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'hdfs:///test_data/input.csv'  #  input HDFS CSV file\n",
    "    output_dir = 'hdfs:///output_data/'  #  output HDFS directory\n",
    "    run_mrjob(input_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7007bc37-261e-4955-8a84-a753d4303943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class PlatformByGender(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Skip the header\n",
    "        if line.startswith(\"User_ID\"):\n",
    "            return\n",
    "\n",
    "        fields = line.split(',')\n",
    "        if len(fields) != 10:\n",
    "            return\n",
    "\n",
    "        user_id, age, gender, platform, daily_usage, posts_per_day, likes_received, comments_received, messages_sent, dominant_emotion = fields\n",
    "\n",
    "        yield (gender, platform), 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        gender, platform = key\n",
    "        yield (gender, platform), sum(values)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PlatformByGender.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb869166-bfe4-4089-aeb9-4b0a3d327f28",
   "metadata": {},
   "source": [
    "## Task 4: Post count based on Platform\n",
    "Mapper: Reads each line of the CSV, extracts gender and Post received, and emits a tuple (gender, post received).\n",
    "\n",
    "Reducer: Aggregates the total likes received for each gender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9312da42-2234-407f-9258-d1423094069e",
   "metadata": {},
   "source": [
    "![Example Image](platform_post_count.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716a999-c33d-492f-8ec5-3125fa3fbace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from platform_by_gender import PlatformByGender\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "# Function to run the MapReduce job\n",
    "def run_mrjob(input_file, output_dir):\n",
    "    # Create a temporary local file to store the intermediate output\n",
    "    temp_output_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    temp_output_filename = temp_output_file.name\n",
    "\n",
    "    try:\n",
    "        # Remove the existing output directory if it exists\n",
    "        subprocess.run(['hdfs', 'dfs', '-rm', '-r', output_dir], check=False)\n",
    "\n",
    "        # Run the MRJob with HDFS input and temporary local output\n",
    "        mr_job = PlatformByGender(args=['-r', 'hadoop', input_file, '--output-dir', output_dir])\n",
    "\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "            # Read the intermediate output from HDFS\n",
    "            subprocess.run(['hdfs', 'dfs', '-getmerge', f'{output_dir}/part-*', temp_output_filename], check=True)\n",
    "\n",
    "            # Write the final output to the desired HDFS path\n",
    "            final_output_path = f'{output_dir}/output_gender.csv'\n",
    "            with open(temp_output_filename, 'r') as infile:\n",
    "                with open('final_output.csv', 'w', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow(['Gender', 'Platform', 'Count'])\n",
    "\n",
    "                    # Read each line and split it by tab ('\\t') to separate key and value\n",
    "                    for line in infile:\n",
    "                        key, value = line.strip().split('\\t')\n",
    "                        gender, platform = eval(key)  # Parse key\n",
    "                        count = int(value)  # Convert value to integer\n",
    "                        writer.writerow([gender, platform, count])\n",
    "\n",
    "            # Put the final output CSV to HDFS\n",
    "            subprocess.run(['hdfs', 'dfs', '-put', 'final_output.csv', final_output_path], check=True)\n",
    "\n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        os.remove(temp_output_filename)\n",
    "        if os.path.exists('final_output.csv'):\n",
    "            os.remove('final_output.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'hdfs:///test_data/input.csv'  #  input HDFS CSV file\n",
    "    output_dir = 'hdfs:///output_data/'  #  output HDFS directory\n",
    "    run_mrjob(input_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40893c7-3b51-4e9e-ad5f-05fdd67fc36b",
   "metadata": {},
   "source": [
    "## Driver code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726da02-5136-4e6a-b905-79e9957c4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from platform_by_gender import PlatformByGender\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "# Function to run the MapReduce job\n",
    "def run_mrjob(input_file, output_dir):\n",
    "    # Create a temporary local file to store the intermediate output\n",
    "    temp_output_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    temp_output_filename = temp_output_file.name\n",
    "\n",
    "    try:\n",
    "        # Remove the existing output directory if it exists\n",
    "        subprocess.run(['hdfs', 'dfs', '-rm', '-r', output_dir], check=False)\n",
    "\n",
    "        # Run the MRJob with HDFS input and temporary local output\n",
    "        mr_job = PlatformByGender(args=['-r', 'hadoop', input_file, '--output-dir', output_dir])\n",
    "\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "            # Read the intermediate output from HDFS\n",
    "            subprocess.run(['hdfs', 'dfs', '-getmerge', f'{output_dir}/part-*', temp_output_filename], check=True)\n",
    "\n",
    "            # Write the final output to the desired HDFS path\n",
    "            final_output_path = f'{output_dir}/output_gender.csv'\n",
    "            with open(temp_output_filename, 'r') as infile:\n",
    "                with open('final_output.csv', 'w', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow(['Gender', 'Platform', 'Count'])\n",
    "\n",
    "                    # Read each line and split it by tab ('\\t') to separate key and value\n",
    "                    for line in infile:\n",
    "                        key, value = line.strip().split('\\t')\n",
    "                        gender, platform = eval(key)  # Parse key\n",
    "                        count = int(value)  # Convert value to integer\n",
    "                        writer.writerow([gender, platform, count])\n",
    "\n",
    "            # Put the final output CSV to HDFS\n",
    "            subprocess.run(['hdfs', 'dfs', '-put', 'final_output.csv', final_output_path], check=True)\n",
    "\n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        os.remove(temp_output_filename)\n",
    "        if os.path.exists('final_output.csv'):\n",
    "            os.remove('final_output.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'hdfs:///test_data/input.csv'  #  input HDFS CSV file\n",
    "    output_dir = 'hdfs:///output_data/'  #  output HDFS directory\n",
    "    run_mrjob(input_file, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
