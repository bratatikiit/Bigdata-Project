{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6ad1f1-b9ab-4e85-a46d-5930267a4a49",
   "metadata": {},
   "source": [
    "## Wir haben zwei Spark installiert.\n",
    "\n",
    "## Auf Windows und WSL Ubuntu 22.04 mit Hadoop/Spark\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a066425-1ac6-481d-857a-89792a022903",
   "metadata": {},
   "source": [
    "In beiden Fällen muß eine Spark Session aufgebaut werden.\n",
    "Danach ist der Code identisch. \n",
    "Die Arbeitsweise auf Spark(W10) und Spark(Hadoop) unterscheidet sich also nur am Anfang im Aufbau der Session.\n",
    "\n",
    "!! Für beide Zugriffe müssen unter Python alle Bibliotheken (Anleitung: 1_alfatraining_RC-PC_W10_alles.docx) installiert sein. !!\n",
    "\n",
    "!! Es kann immer nur eine Spark-Session offen sein !! Mit spark.stop() die letzzte Session beenden !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54243c91-d9d6-48fc-83ee-83d418fee7eb",
   "metadata": {},
   "source": [
    "### Sessionaufbau für Spark-Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9472e468-3f9c-4741-91cf-b5b5fd4511be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"MeinProgramm.com\") \\\n",
    "      .getOrCreate() \n",
    "\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6032077-4a85-4ddd-ab49-3b58ea4beb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b89c6-51bb-4327-b699-a53c47c0aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Beispiel einlesen (read einer CSV, die unter Windows gespeichert ist, im selben Ordner wie dieses Notebook)\n",
    "\n",
    "Sdf1 = spark.read.csv(\"all_us_zipcodes.csv\")\n",
    "\n",
    "print(Sdf1.printSchema())\n",
    "print(Sdf1.show())\n",
    "\n",
    "# oder\n",
    "\n",
    "Sdf2 = spark.read.format('csv').load(\"zipcode.csv\")\n",
    "print(Sdf2.printSchema())\n",
    "print(Sdf2.show())\n",
    "\n",
    "print(type(Sdf2))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfe9712a-78ab-4723-bd03-f7a4b7ddd1ac",
   "metadata": {},
   "source": [
    "Jetzt kann alles was mit pyspark möglich ist ausgeführt werden.\n",
    "(Ausnahme: RDD-API (noch nicht verifiziert))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8b181-da84-43e0-a13b-93b6bc04ba66",
   "metadata": {},
   "source": [
    "### Schreiben eines Spark DataFrame in eine Datei unter Windows"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5d0b7ee-3de6-4bf2-ba91-d6372baeec66",
   "metadata": {},
   "source": [
    "Sdf1.write.mode(\"overwrite\").csv(output_path, header=True)\n",
    "\n",
    "Dieser Befehl ist für Hadoop optimiert.\n",
    "\n",
    "Der output_path enthält nicht den Dateinamen. \n",
    "Spark schreibt den DataFrame in mehrere Dateien in einem Verzeichnis, \n",
    "da es für verteilte Datenverarbeitung optimiert ist. \n",
    "\n",
    "Wenn du den DataFrame in einer einzigen CSV-Datei speichern möchtest, \n",
    "kannst du zuerst den DataFrame in einen Pandas DataFrame konvertieren \n",
    "und dann die Pandas Methode verwenden, um den DataFrame als CSV-Datei zu speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63c858-f21c-4746-ac13-726ea87015ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiere die notwendigen Bibliotheken\n",
    "import pandas as pd\n",
    "\n",
    "# Konvertiere den Spark DataFrame in einen Pandas DataFrame\n",
    "pandas_df = Sdf1.toPandas()\n",
    "\n",
    "# Speichere den Pandas DataFrame als eine einzelne CSV-Datei\n",
    "output_path = \"test.csv\"  # im gleichen Ordner wie das Notebook\n",
    "# oder mit Pfadangabe\n",
    "# output_path = \"c:\\\\0_BigData\\\\test.csv\"\n",
    "\n",
    "pandas_df.to_csv(output_path, index=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccdcefd-9e9d-49c1-821c-5e15adb8edc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3b9c0dd1-bdb6-4fa3-8020-14bcdb948c58",
   "metadata": {},
   "source": [
    "Wenn der Spark DataFrame zu groß für Pandas ist, \n",
    "kannst du den DataFrame in mehreren Teilen speichern, \n",
    "indem du einen Generator verwendest. \n",
    "Dies ist ein Ansatz, der es dir ermöglicht, die Daten schrittweise zu verarbeiten und zu speichern, \n",
    "ohne den gesamten DataFrame gleichzeitig im Speicher zu halten. \n",
    "\n",
    "Hier ist ein Beispiel, wie du dies tun kannst:\n",
    "\n",
    "1. Schreibe den Spark DataFrame in Partitionen.\n",
    "2. Verwende einen Generator, um die Partitionen zu lesen und zu speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a797b0d9-17f5-4d1b-affa-c7fdcc8770a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Der Code liefert Fehler:\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"MeinProgramm.com\") \\\n",
    "      .getOrCreate() \n",
    "\n",
    "# Lies den Spark DataFrame ein\n",
    "sdf1 = spark.read.csv(\"all_us_zipcodes.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Definiere den Ausgabeordner\n",
    "output_folder = \"c:\\\\0_BigData\\\\Source\"\n",
    "\n",
    "# Stelle sicher, dass der Ausgabeordner existiert\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Schreibe den Spark DataFrame in Partitionen\n",
    "partitioned_path = os.path.join(output_folder, \"partitioned_data\")\n",
    "\n",
    "sdf1.repartition(2).write.csv(partitioned_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Generator, um die Partitionen zu lesen und zu einer einzigen Datei zusammenzuführen\n",
    "def merge_partitions_to_single_csv(partitioned_path, output_file):\n",
    "    # Suche alle Partitionen\n",
    "    partition_files = [os.path.join(partitioned_path, f) for f in os.listdir(partitioned_path) if f.startswith(\"part-\")]\n",
    "    \n",
    "    # Schreibe die Daten von allen Partitionen in eine einzelne Datei\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for i, partition_file in enumerate(partition_files):\n",
    "            with open(partition_file, 'r', encoding='utf-8') as infile:\n",
    "                for line in infile:\n",
    "                    if i != 0 and infile.tell() == 0:\n",
    "                        # Überspringe die Headerzeile nach der ersten Partition\n",
    "                        continue\n",
    "                    outfile.write(line)\n",
    "\n",
    "# Definiere den endgültigen Dateinamen\n",
    "final_output_file = os.path.join(output_folder, \"final_output.csv\")\n",
    "\n",
    "# Führe die Partitionen zusammen\n",
    "merge_partitions_to_single_csv(partitioned_path, final_output_file)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba85e5cc-e807-4e50-973c-f3cc85f64e0f",
   "metadata": {},
   "source": [
    "1. Der Spark DataFrame wird in 10 Partitionen geschrieben (sdf1.repartition(10).write.csv(...)). \n",
    "   Du kannst die Anzahl der Partitionen anpassen.\n",
    "\n",
    "2. Die Funktion merge_partitions_to_single_csv liest die Partitionen und schreibt sie in eine einzelne CSV-Datei.   \n",
    "   Diese Funktion überspringt die Headerzeilen nach der ersten Partition, \n",
    "   um doppelte Headerzeilen in der finalen CSV-Datei zu vermeiden.\n",
    "\n",
    "3. Der finale Dateipfad wird als final_output_file definiert, \n",
    "   und die Partitionen werden zusammengeführt.\n",
    "\n",
    "Dieser Ansatz ermöglicht es dir, \n",
    "einen großen Spark DataFrame effizient in eine einzelne CSV-Datei zu speichern, \n",
    "ohne dass der gesamte DataFrame gleichzeitig im Speicher gehalten werden muss.\n",
    "\n",
    "################# Fehlerbegründung:#####################################################\n",
    "\n",
    "Der Fehler, den du siehst, wird durch ein Problem mit den nativen Bibliotheken von Hadoop auf Windows verursacht. Dieses Problem tritt häufig auf, wenn Spark auf einem Windows-System ausgeführt wird und versucht, mit dem lokalen Dateisystem zu interagieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55393d6d-9ee9-4843-8044-b2c947a67e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "553da0d0-5867-485c-8264-731bd85702cb",
   "metadata": {},
   "source": [
    "# Sessionaufbau für Spark-Hadoop auf Ubuntu"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49bb44d2-2d79-4a2a-bff2-8965ca03d304",
   "metadata": {},
   "source": [
    "Vorraussetzungen hierfür sind:\n",
    "\n",
    "- Gestarteter zkServer und Hadoop\n",
    "\n",
    "- Unter Ubuntu muß mit folgendem Befehl der Connect-Server gestartet werden\n",
    "\n",
    "  start-connect-server.sh --packages org.apache.spark:spark-connect_2.13:3.5.1\n",
    "\n",
    "  Die 3.5.1 am Ende ist die installierte Spark-Version\n",
    "\n",
    "  Man sollte 1-2 Minuten warten bist der Server gestartet ist.\n",
    "\n",
    "  mit jps kann man das testen und sehen:\n",
    "\n",
    "                     YarnCoarseGrainedExecutorBackend    (2x mit anderen Prozess-ID)\n",
    "                     ExecutorLauncher\n",
    "                     SparkSubmit\n",
    "\n",
    "  sollten zu sehen sein.\n",
    "  \n",
    "  Wenn der Spark-Zugriff beendet ist kann man den Connect-Server mit dem folgenden Befehl beenden:\n",
    "\n",
    "  stop-connect-server.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32471f83-f3bf-474e-a248-408bceb32f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install grpcio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ea177-cce5-414b-aaa8-3ac38176b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install grpcio-status --use-deprecated=legacy-resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87b10c-2899-4f06-bd28-ffa6606530f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-api-core --use-deprecated=legacy-resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7280bff0-a7fb-4f43-83c6-e70568f46953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.connect.session.SparkSession"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jetzt kann eine Session mit Spark auf Hadoop erstellt werden\n",
    "# Hierzu benötigen wir wieder die IP-Adresse des WSL-Ubuntu\n",
    "# Zu finden auf Ubuntu mit  hostname -I\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.remote(\"sc://172.21.82.235:15002\") \\\n",
    "                            .appName(\"MeinProgramm.com\") \\\n",
    "                            .getOrCreate()\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a98f8d-d89b-4480-b681-9c621d633240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.sleep(10)  # Wait for 10 seconds\n",
    "\n",
    "spark = SparkSession.builder.remote(\"sc://172.21.82.235:15002\") \\\n",
    "                    .appName(\"MeinProgramm.com\") \\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5abcd8-70f5-4c4f-8d96-40470b5e459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b37f0-765f-420e-adc8-690d3f1d75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Beispiel einlesen (read einer CSV, die im HDFS gespeichert ist)\n",
    "\n",
    "# Sdf = spark.read.csv(\"all_us_zipcodes.csv\")\n",
    "# Problem: So erwartet er die Datei nicht in der hdfs-Root, sondern unter /user/alfa\n",
    "\n",
    "# Lies die Datei aus einem beliebigen Ordner im HDFS\n",
    "hdfs_path = \"hdfs://172.26.195.52:9000/bigdata/test123.csv\"\n",
    "Sdf = spark.read.csv(hdfs_path)\n",
    "\n",
    "print(Sdf.printSchema())\n",
    "print(Sdf.show())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19e01e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6c354-d40e-4101-9572-2fe502000724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreiben eines Spark DataFrame in Hadoop hdfs\n",
    "\n",
    "output_path = \"/bigdata\"\n",
    "\n",
    "Sdf.write.mode(\"overwrite\").csv(output_path, header=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1aad9c4e-6d43-49e8-897e-5128fea2ab57",
   "metadata": {},
   "source": [
    "Der Befehl erstellt im HDFS eine Datei, die in mehrere Partitionen aufgeteilt ist.\n",
    "Man kann keinen Dateinamen angeben.\n",
    "\n",
    "Um das zu machen verwendfen wir die bekannte Bibliothek hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912c12b-9f44-486c-bf1a-2ecff8594076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# Remote SparkSession erstellen\n",
    "spark = SparkSession.builder.remote(\"sc://172.26.195.52:15002\") \\\n",
    "                    .appName(\"MeinProgramm.com\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "Sdf = spark.read.csv(\"all_us_zipcodes.csv\")\n",
    "# Problem: So erwartet er die Datei nicht in der hdfs-Root, sondern unter /user/alfa\n",
    "# Verzeichnispfad\n",
    "output_path = \"/bigdata/temp_output\"\n",
    "\n",
    "# aufgeteilten DataFrame inj einer Datei schreiben\n",
    "'''\n",
    "Wir verwenden coalesce(1), um sicherzustellen, \n",
    "dass der DataFrame in einer einzigen Partition geschrieben wird, \n",
    "was in einer einzelnen Datei resultiert. \n",
    "'''\n",
    "Sdf.coalesce(1).write.mode(\"overwrite\").csv(output_path, header=True)\n",
    "\n",
    "# HDFS-Client erstellen\n",
    "hdfs_url        = 'http://172.26.195.52:9870'\n",
    "ubuntu_Benutzer = 'alfa'\n",
    "client = InsecureClient(hdfs_url, user=ubuntu_Benutzer)\n",
    "\n",
    "# Dateien im HDFS-Verzeichnis auflisten\n",
    "files = client.list(output_path)\n",
    "csv_file = [file for file in files if file.endswith(\".csv\")][0]\n",
    "\n",
    "# Quell- und Zielpfad für die umbenannte Datei\n",
    "source_path = f\"{output_path}/{csv_file}\"\n",
    "target_path = \"/bigdata/test1234.csv\"\n",
    "\n",
    "# Datei umbenennen\n",
    "client.rename(source_path, target_path)\n",
    "\n",
    "print(f\"Datei wurde unter {target_path} gespeichert\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c03cd4-c9ed-4543-8f6b-481fa8075220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
