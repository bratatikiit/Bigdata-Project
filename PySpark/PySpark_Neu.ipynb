{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adadbc09-0628-44ba-a0ce-d1475fe24262",
   "metadata": {},
   "source": [
    "PySpark RDD – Robustes verteiltes Dataset\r\n",
    "In diesem Abschnitt des PySpark-Tutorials werde ich die RDD vorstellen und anhand von Beispielen erklären, wie sie erstellt und ihre Transformations- und Aktionsvorgänge verwendet werden. Hier ist der vollständige Artikel über PySpark RDD, falls Sie mehr darüber erfahren und Ihre Grundlagen stärken möchten.\r\n",
    "\r\n",
    "PySpark RDD (Resilient Distributed Dataset) ist eine grundlegende Datenstruktur von PySpark, bei der es sich um fehlertolerante, unveränderliche verteilte Sammlungen von Objekten handelt, was bedeutet, dass Sie ein RDD nicht mehr ändern können, sobald Sie es erstellt haben. Jedes Dataset in RDD ist in logische Partitionen unterteilt, die auf verschiedenen Knoten des Clusters berechnet werden können.\r\n",
    "\r\n",
    "RDD-Erstellung\r\n",
    "Um eine RDD zu erstellen, müssen Sie zunächst eine SparkSession erstellen, die ein Einstiegspunkt für die PySpark-Anwendung ist. SparkSession kann mit einer oder-Methode der SparkSession erstellt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07308cd1-3cb3-45ca-84c8-3e0ffe943f09",
   "metadata": {},
   "source": [
    "Die Spark-Sitzung erstellt intern die Variable . Sie können mehrere SparkSession-Objekte erstellen, aber nur einen SparkContext pro JVM. \n",
    "Wenn Sie einen weiteren neuen SparkContext erstellen möchten, \n",
    "sollten Sie den vorhandenen Sparkcontext (mit ) beenden, bevor Sie einen neuen erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae98c36-5428-4742-8474-43cdf468582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .getOrCreate() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77512254-2617-408a-89d5-8f525b9e220e",
   "metadata": {},
   "source": [
    "Verwenden von parallelize()\r\n",
    "SparkContext verfügt über mehrere Funktionen, die mit RDDs verwendet werden können. Zum Beispiel wird seine Methode verwendet, um eine RDD aus einer Liste zu erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1bc3d9-da14-470a-bc78-53968e3e69a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD from parallelize    \n",
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd=spark.sparkContext.parallelize(dataList)\n",
    "rdd.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da194e6-38f3-4182-9e54-48efabfd9a1f",
   "metadata": {},
   "source": [
    "Verwendung von textFile()\r\n",
    "RDD kann auch aus einer Textdatei erstellt werden, indem die Funktion des SparkContext verwendet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68359377-5cd7-4bc3-a94f-2295528640e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD from external Data source\n",
    "rdd2 = spark.sparkContext.textFile(\"datacamp_ecommerce.csv\")\n",
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b198c7-0cf2-46d9-ac86-9873ba29d7ca",
   "metadata": {},
   "source": [
    "Sobald Sie über eine RDD verfügen, können Sie Transformations- und Aktionsvorgänge ausführen. \n",
    "Alle Vorgänge, die Sie auf RDD ausführen, werden parallel ausgeführt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a46945-283d-4851-b709-31f9c2f4012e",
   "metadata": {},
   "source": [
    "RDD-Vorgänge\r\n",
    "Auf PySpark RDD können Sie zwei Arten von Vorgängen ausführen.\r\n",
    "\r\n",
    "RDD-Transformationen – Transformationen sind faule Vorgänge. Wenn Sie eine Transformation (z. B. Update) ausführen, geben diese Vorgänge anstelle des Aktualisierens einer aktuellen RDD eine andere RDD zurü\n",
    "RDD-Aktionen – Vorgänge, die Berechnungen auslösen und RDD-Werte an den Treiber zurückgeben.\n",
    "ck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42554868-8c4c-43a3-b0e6-2f189775063c",
   "metadata": {},
   "source": [
    "RDD-Aktionen\r\n",
    "RDD-Aktionsvorgang gibt die Werte von einem RDD an einen Treiberknoten zurück. Mit anderen Worten: Jede RDD-Funktion, die Nicht-RDD[T] zurückgibt, wird als Aktion betrachtet.\r\n",
    "\r\n",
    "Einige Aktionen für RDDs sind , , , und mehr.count()collect()first()max()reduce()\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775481b6-8b07-43ef-bdbc-426dd5a6e61d",
   "metadata": {},
   "source": [
    "PySpark DataFrame-Tutorial für Anfänger\r\n",
    "Die DataFrame-Definition wird von Databricks sehr gut erklärt, daher möchte ich sie nicht erneut definieren und Sie verwirren. Im Folgenden finden Sie die Definition, die ich von Databricks übernommen habe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390e53e-be80-4d89-87db-bb7512e4ec81",
   "metadata": {},
   "source": [
    "DataFrame ist eine verteilte Sammlung von Daten, die in benannten Spalten organisiert sind. \n",
    "Sie entspricht konzeptionell einer Tabelle in einer relationalen Datenbank oder einem Datenrahmen in R/Python, jedoch mit umfangreicheren Optimierungen unter der Haube. \n",
    "DataFrames können aus einer Vielzahl von Quellen erstellt werden, z. B. aus strukturierten Datendateien, Tabellen in Hive, externen Datenbanken oder vorhandenen RDDs.\n",
    "\n",
    "– Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6094ac8-6491-4ad0-ad6b-cf3bcff856e1",
   "metadata": {},
   "source": [
    "Wenn Sie einen Python-Hintergrund haben, würde ich davon ausgehen, dass Sie bereits wissen, was Pandas DataFrame ist. PySpark DataFrame ähnelt größtenteils Pandas DataFrame, mit der Ausnahme, dass PySpark DataFrames im Cluster verteilt sind (d. h. die Daten in Datenrahmen werden auf verschiedenen Computern in einem Cluster gespeichert) und alle Vorgänge in PySpark parallel auf allen Computern ausgeführt werden, während Panda Dataframe auf einem einzelnen Computer speichert und ausgeführt wird.\n",
    "\n",
    "Wenn Sie keinen Python-Hintergrund haben, würde ich Ihnen empfehlen, einige Grundlagen von Python zu lernen, bevor Sie mit diesem Spark-Tutorial fortfahren. Für den Moment sollten Sie nur wissen, dass Daten in PySpark-DataFrames auf verschiedenen Computern in einem Cluster gespeichert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242cf536-5711-41d8-9341-f783c2735bff",
   "metadata": {},
   "source": [
    "***DataFrame-Erstellung***\n",
    "\r\n",
    "Die einfachste Möglichkeit, einen DataFrame zu erstellen, besteht darin, eine Python-Datenliste zu erstellen. DataFrame kann auch aus einer RDD und durch Lesen von Dateien aus mehreren Quellen erstellt werden.\r\n",
    "\r\n",
    "Verwenden von createDataFrame()\r\n",
    "Mit Hilfe der Funktion der SparkSession können Sie einen DataFrame erstellen.createDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "496cec6e-0e8f-478f-a5af-32cf23b3c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- _6: long (nullable = true)\n",
      "\n",
      "None\n",
      "+---------+----------+--------+----------+------+----+\n",
      "|firstname|middlename|lastname|    gender|salary|  _6|\n",
      "+---------+----------+--------+----------+------+----+\n",
      "|    James|          |   Smith|1991-04-01|     M|3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|  -1|\n",
      "+---------+----------+--------+----------+------+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "print(df.printSchema())\n",
    "print(df.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c3f8d1-4cee-4901-a8ee-902162db2184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.csv(\"datacamp_ecommerce.csv\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902fd032-30ae-4654-8af0-48711a520643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+---------+------+-----------+-------------+----------+-------------+-----------+-------------------+---------------+----------+-----------------+------------------+----------+--------------------+----------+--------+\n",
      "|           _c0|        _c1|       _c2|      _c3|   _c4|        _c5|          _c6|       _c7|          _c8|        _c9|               _c10|           _c11|      _c12|             _c13|              _c14|      _c15|                _c16|      _c17|    _c18|\n",
      "+--------------+-----------+----------+---------+------+-----------+-------------+----------+-------------+-----------+-------------------+---------------+----------+-----------------+------------------+----------+--------------------+----------+--------+\n",
      "|Transaction_id|customer_id|      Date|  Product|Gender|Device_Type|      Country|     State|         City|   Category|Customer_Login_type|  Delivery_Type| Quantity |Transaction Start|Transaction_Result|Amount US$|Individual_Price_US$|Year_Month|    Time|\n",
      "|         40170| 1348959766|14/11/2013|Hair Band|Female|        Web|United States|  New York|New York City|Accessories|             Member|one-day deliver|        12|                1|                 0|     6,910|                 576|    13-Nov|22:35:51|\n",
      "|         33374| 2213674919|05/11/2013|Hair Band|Female|        Web|United States|California|   Los Angles|Accessories|             Member|one-day deliver|        17|                1|                 1|     1,699|                 100|    13-Nov|06:44:41|\n",
      "|         14407| 1809450308|01/10/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|Normal Delivery|        23|                1|                 0|     4,998|                 217|    13-Oct|00:41:24|\n",
      "|         15472| 1691227134|04/10/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|Normal Delivery|        23|                1|                 0|       736|                  32|    13-Oct|22:04:03|\n",
      "|         18709| 2290737237|12/10/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|Normal Delivery|        23|                1|                 1|     4,389|                 191|    13-Oct|15:00:46|\n",
      "|         19141| 2166771669|13/10/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|Normal Delivery|        19|                1|                 0|     1,798|                  95|    13-Oct|18:09:02|\n",
      "|         19922| 2134863151|15/10/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|one-day deliver|        20|                1|                 1|       124|                   6|    13-Oct|23:17:59|\n",
      "|         20737| 1183410853|16/10/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|one-day deliver|        14|                1|                 1|         0|                   0|    13-Oct|19:17:39|\n",
      "|         21910| 1849457093|18/10/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|one-day deliver|        20|                1|                 1|     1,000|                  50|    13-Oct|00:03:47|\n",
      "|         23454| 2010052101|20/10/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|Normal Delivery|        23|                1|                 1|       602|                  26|    13-Oct|20:52:24|\n",
      "|         24718| 1330443392|22/10/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|one-day deliver|        20|                1|                 0|    21,500|               1,075|    13-Oct|18:09:41|\n",
      "|         25749| 1864893410|24/10/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|one-day deliver|        20|                1|                 1|     1,000|                  50|    13-Oct|11:19:51|\n",
      "|         26498| 1623523846|25/10/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|one-day deliver|        20|                1|                 1|         0|                   0|    13-Oct|13:48:08|\n",
      "|         28225| 1456549032|28/10/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|Normal Delivery|        23|                1|                 1|         0|                   0|    13-Oct|08:09:10|\n",
      "|         29306| 2276213857|30/10/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|one-day deliver|        14|                1|                 1|     4,900|                 350|    13-Oct|09:39:01|\n",
      "|         30992| 1521344294|01/11/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|Normal Delivery|        15|                1|                 1|     3,003|                 200|    13-Nov|11:05:12|\n",
      "|         32839| 1699850696|04/11/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|one-day deliver|        20|                1|                 1|    25,635|               1,282|    13-Nov|09:17:48|\n",
      "|         34993| 1653948384|07/11/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|Normal Delivery|        22|                1|                 1|       500|                  23|    13-Nov|10:25:27|\n",
      "|         35184| 1475591045|08/11/2013|Hair Band|Female|        Web|United States|Washington|      Seattle|Accessories|             Member|one-day deliver|        20|                1|                 0|     7,926|                 396|    13-Nov|17:03:30|\n",
      "+--------------+-----------+----------+---------+------+-----------+-------------+----------+-------------+-----------+-------------------+---------------+----------+-----------------+------------------+----------+--------------------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92b0eb-083c-4a96-8764-1e3d101ba88c",
   "metadata": {},
   "source": [
    "PySpark withColumnRenamed zum Umbenennen der Spalte im DataFrame\n",
    "\n",
    "Verwenden Sie PySpark, um eine DataFrame-Spalte umzubenennen, müssen wir häufig eine Spalte oder mehrere (oder alle) Spalten im PySpark-DataFrame umbenennen, Sie können dies auf verschiedene Arten tun. Wenn Spalten verschachtelt sind, wird es kompliziert.    withColumnRenamed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eff136c2-4048-4af3-ba97-a6cc3d3e7cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c8fee7-4eff-48a1-917d-c8d72f3aef4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"zipcode.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac4feb1d-102a-478c-8db7-1e1999333805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "None\n",
      "+--------------------+----------+------+------+\n",
      "|                name|       dob|gender|salary|\n",
      "+--------------------+----------+------+------+\n",
      "|    {James, , Smith}|1991-04-01|     M|  3000|\n",
      "|   {Michael, Rose, }|2000-05-19|     M|  4000|\n",
      "|{Robert, , Williams}|1978-09-05|     M|  4000|\n",
      "|{Maria, Anne, Jones}|1967-12-01|     F|  4000|\n",
      "|  {Jen, Mary, Brown}|1980-02-17|     F|    -1|\n",
      "+--------------------+----------+------+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
    "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
    "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
    "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
    "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('dob', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "df = spark.createDataFrame(data = dataDF, schema = schema)\n",
    "print(df.printSchema())\n",
    "print(df.show())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50383c0c-0ba9-4b77-b5c8-f503cba572ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- Geburtstag: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"dob\",\"Geburtstag\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d5f8b5e-b217-4101-8d7d-042b3906b39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- Geb.-Tag: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- Verkauf: integer (nullable = true)\n",
      "\n",
      "+--------------------+----------+------+-------+\n",
      "|                name|  Geb.-Tag|gender|Verkauf|\n",
      "+--------------------+----------+------+-------+\n",
      "|    {James, , Smith}|1991-04-01|     M|   3000|\n",
      "|   {Michael, Rose, }|2000-05-19|     M|   4000|\n",
      "|{Robert, , Williams}|1978-09-05|     M|   4000|\n",
      "|{Maria, Anne, Jones}|1967-12-01|     F|   4000|\n",
      "|  {Jen, Mary, Brown}|1980-02-17|     F|     -1|\n",
      "+--------------------+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumnRenamed(\"dob\",\"Geb.-Tag\") \\\n",
    "    .withColumnRenamed(\"salary\",\"Verkauf\")\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fedd1c47-5f77-4ea5-a966-739d68ebb7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- fname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"fname\",StringType()),\n",
    "    StructField(\"middlename\",StringType()),\n",
    "    StructField(\"lname\",StringType())])\n",
    "\n",
    "\n",
    "df.select(col(\"name\").cast(schema2), \\\n",
    "     col(\"dob\"), col(\"gender\"),col(\"salary\")) \\\n",
    "   .printSchema()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3533d89d-9812-43d9-8fec-735ad96ef6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- fname: string (nullable = true)\n",
      " |-- mname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      "\n",
      "+----------+------+------+-------+-----+--------+\n",
      "|       dob|gender|salary|  fname|mname|   lname|\n",
      "+----------+------+------+-------+-----+--------+\n",
      "|1991-04-01|     M|  3000|  James|     |   Smith|\n",
      "|2000-05-19|     M|  4000|Michael| Rose|        |\n",
      "|1978-09-05|     M|  4000| Robert|     |Williams|\n",
      "|1967-12-01|     F|  4000|  Maria| Anne|   Jones|\n",
      "|1980-02-17|     F|    -1|    Jen| Mary|   Brown|\n",
      "+----------+------+------+-------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df4 = df.withColumn(\"fname\",col(\"name.firstname\")) \\\n",
    "      .withColumn(\"mname\",col(\"name.middlename\")) \\\n",
    "      .withColumn(\"lname\",col(\"name.lastname\")) \\\n",
    "      .drop(\"name\")\n",
    "df4.printSchema()\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f00c3-b75f-444f-a518-a3a02a83a19f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
