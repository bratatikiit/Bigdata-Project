{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5e52a-4ba4-421f-9012-8246ccf8a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39ea448-f244-4f0f-9d55-e386b81a034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1435c726-8132-493d-9d88-bcad13423d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 206 entries, 0 to 205\n",
      "Data columns (total 10 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   User_ID                     103 non-null    float64\n",
      " 1   Age                         103 non-null    object \n",
      " 2   Gender                      103 non-null    object \n",
      " 3   Platform                    103 non-null    object \n",
      " 4   Daily_Usage_Time (minutes)  103 non-null    float64\n",
      " 5   Posts_Per_Day               103 non-null    float64\n",
      " 6   Likes_Received_Per_Day      103 non-null    float64\n",
      " 7   Comments_Received_Per_Day   103 non-null    float64\n",
      " 8   Messages_Sent_Per_Day       103 non-null    float64\n",
      " 9   Dominant_Emotion            103 non-null    object \n",
      "dtypes: float64(6), object(4)\n",
      "memory usage: 16.2+ KB\n",
      "None\n",
      "\n",
      "First few rows of the dataframe:\n",
      "   User_ID  Age      Gender  Platform  Daily_Usage_Time (minutes)  \\\n",
      "0      NaN  NaN         NaN       NaN                         NaN   \n",
      "1    500.0   27      Female  Snapchat                       120.0   \n",
      "2      NaN  NaN         NaN       NaN                         NaN   \n",
      "3    488.0   21  Non-binary  Snapchat                        60.0   \n",
      "4      NaN  NaN         NaN       NaN                         NaN   \n",
      "\n",
      "   Posts_Per_Day  Likes_Received_Per_Day  Comments_Received_Per_Day  \\\n",
      "0            NaN                     NaN                        NaN   \n",
      "1            4.0                    40.0                       18.0   \n",
      "2            NaN                     NaN                        NaN   \n",
      "3            1.0                    18.0                        7.0   \n",
      "4            NaN                     NaN                        NaN   \n",
      "\n",
      "   Messages_Sent_Per_Day Dominant_Emotion  \n",
      "0                    NaN              NaN  \n",
      "1                   22.0          Neutral  \n",
      "2                    NaN              NaN  \n",
      "3                   12.0          Neutral  \n",
      "4                    NaN              NaN  \n"
     ]
    }
   ],
   "source": [
    "# Display basic information and identify issues\n",
    "print(\"Dataframe Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows of the dataframe:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a2409-b4a9-40d0-aa44-16920f749939",
   "metadata": {},
   "source": [
    "## Data Cleaning Script for Social Media User Data \n",
    "\n",
    "# This script cleans a CSV file containing social media user data. It performs the following tasks:\n",
    "\n",
    "Load Data: Imports the pandas library and reads the CSV file into a DataFrame named df.\n",
    "Data Exploration:\n",
    "Prints basic information about the DataFrame using .info().\n",
    "Prints the first few rows using .head() to identify potential issues.\n",
    "# Missing Values:\n",
    "Prints the number of missing values per column using .isnull().sum().\n",
    "Drops rows with any missing values using .dropna() and stores the cleaned data in df_cleaned.\n",
    "Data Type Conversion:\n",
    "Iterates through specific columns and attempts to convert their values to numeric data types using .to_numeric().\n",
    "Uses the errors='coerce' argument to handle non-numeric values by converting them to NaN (Not a Number).\n",
    "Updates the data types in-place using .loc for better memory efficiency.\n",
    "Drops rows with conversion errors using .dropna().\n",
    "# Duplicate Removal:\n",
    "Removes duplicate rows from df_cleaned using .drop_duplicates().\n",
    "# Outlier Detection and Removal:\n",
    "Imports the zscore function from scipy.stats for z-score calculation.\n",
    "Defines a function remove_outliers that calculates z-scores for a specified column.\n",
    "# Filters the DataFrame to keep rows with z-scores within 3 standard deviations of the mean, effectively removing outliers.\n",
    "Drops the temporary zscore column after filtering.\n",
    "Applies the remove_outliers function to relevant columns containing numeric data (e.g., Daily_Usage_Time, Posts_Per_Day).\n",
    "# Saving Cleaned Data:\n",
    "Saves the cleaned DataFrame df_cleaned to a new CSV file named cleaned_file.csv using .to_csv().\n",
    "Sets index=False to avoid saving the row index in the output file.\n",
    "Confirmation:\n",
    "Prints basic information about the cleaned DataFrame using .info() for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5e901-6446-4e5a-8fb7-d6b6eec1ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('./test.csv')\n",
    "\n",
    "# Display basic information and identify issues\n",
    "print(\"Dataframe Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows of the dataframe:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Convert columns to appropriate data types using .loc\n",
    "df_cleaned.loc[:, 'Age'] = pd.to_numeric(df_cleaned['Age'], errors='coerce')\n",
    "df_cleaned.loc[:, 'Daily_Usage_Time (minutes)'] = pd.to_numeric(df_cleaned['Daily_Usage_Time (minutes)'], errors='coerce')\n",
    "df_cleaned.loc[:, 'Posts_Per_Day'] = pd.to_numeric(df_cleaned['Posts_Per_Day'], errors='coerce')\n",
    "df_cleaned.loc[:, 'Likes_Received_Per_Day'] = pd.to_numeric(df_cleaned['Likes_Received_Per_Day'], errors='coerce')\n",
    "df_cleaned.loc[:, 'Comments_Received_Per_Day'] = pd.to_numeric(df_cleaned['Comments_Received_Per_Day'], errors='coerce')\n",
    "df_cleaned.loc[:, 'Messages_Sent_Per_Day'] = pd.to_numeric(df_cleaned['Messages_Sent_Per_Day'], errors='coerce')\n",
    "\n",
    "# Drop rows with conversion errors\n",
    "df_cleaned = df_cleaned.dropna()\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "\n",
    "# Define a function to remove outliers based on z-score\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def remove_outliers(df, column):\n",
    "    df['zscore'] = zscore(df[column])\n",
    "    df = df[abs(df['zscore']) < 3]\n",
    "    df = df.drop(columns=['zscore'])\n",
    "    return df\n",
    "\n",
    "# Apply the function to relevant columns\n",
    "df_cleaned = remove_outliers(df_cleaned, 'Daily_Usage_Time (minutes)')\n",
    "df_cleaned = remove_outliers(df_cleaned, 'Posts_Per_Day')\n",
    "df_cleaned = remove_outliers(df_cleaned, 'Likes_Received_Per_Day')\n",
    "df_cleaned = remove_outliers(df_cleaned, 'Comments_Received_Per_Day')\n",
    "df_cleaned = remove_outliers(df_cleaned, 'Messages_Sent_Per_Day')\n",
    "\n",
    "# Save the cleaned dataframe to a new CSV file\n",
    "df_cleaned.to_csv('cleaned_file.csv', index=False)\n",
    "\n",
    "print(\"\\nDataframe after cleaning and saving:\")\n",
    "print(df_cleaned.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57dc02-f44a-457a-b26a-7ff8b0ab3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mrjob pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f649214-52c8-4d52-a4e9-94987e8c66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mrjob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b021c-7dda-49fc-8507-124c1e66dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ccf05a-3e28-49bf-9798-df50a1d45b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Assuming your CSV data is saved as 'social_media_data.csv'\n",
    "data_df = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Print the first few rows to confirm data loading\n",
    "data_df.show(5)  # Show the first 5 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d29c2-64c5-4b6e-bf2f-1fb4834bcd81",
   "metadata": {},
   "source": [
    "# **Filtering Data:**\n",
    "\n",
    "## Filter rows based on specific conditions using the filter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d39ee-d371-4fd9-ba67-342b1578d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filtered_df.filter(data_df.Age > 25)  # Select rows where Age is greater than 25\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102bf750-a2d4-4fad-9fb8-aa5a13b1cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "selected_data = filtered_data.select(\"User_ID\",\"Daily_Usage_Time (minutes)\",\"Likes_Received_Per_Day\")\n",
    "print(\"Selected Columns User_ID,Daily_Usage_Time (minutes), Likes_Received_Per_Day\")\n",
    "selected_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff3f0e-6538-46f7-a9a6-0a482febefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group By and Aggregation (replace columns as needed)\n",
    "avg_usage_by_platform = filtered_data.groupBy(\"Platform\").agg(mean(\"Daily_Usage_Time (minutes)\").alias(\"Avg_Daily_Usage_Time\"))\n",
    "print(\"Average Daily Usage Time per Platform:\")\n",
    "avg_usage_by_platform.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37040c9-f80a-4b8b-87e1-67b53f59b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistics (replace columns as needed)\n",
    "print(\"Descriptive Statistics (Daily_Usage_Time, Age, Posts_Per_Day):\")\n",
    "data_df.describe(\"Daily_Usage_Time (minutes)\", \"Age\", \"Posts_Per_Day\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b56df-7fbd-475e-a7a2-4f2987c90e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis and UDFs can be included here, showing output after each step\n",
    "\n",
    "# Stop the SparkSession (local environment)\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35df21b-3235-441d-a0e4-09e342bb6c6d",
   "metadata": {},
   "source": [
    "**My input file is input.csv.in WSL Ubuntu 22.4 i run dominant_emotion.py ,below is mapreduce code.**\n",
    "\n",
    "hdfs dfs -put /tmp/input.csv /test_data/input.csv .I run this command in terminal to put it hdfs and run it in hadoop **python3 run_job.py** i store output getting from Hadoop in Hdfs .i download output.csv from HDFS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490d81f-a988-45fc-89eb-77ee2d619043",
   "metadata": {},
   "source": [
    "TASK 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b0164-a0ac-49f9-b824-99cbd1c54c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapreduce job\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class DominantEmotion(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_emotions,\n",
    "                   reducer=self.reducer_count_emotions)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_emotions(self, _, line):\n",
    "        # Skip the header\n",
    "        if line.startswith('User_ID'):\n",
    "            return\n",
    "\n",
    "        parts = line.split(',')\n",
    "\n",
    "        # Ensure there are enough parts in the line to avoid index errors\n",
    "        if len(parts) > 9:\n",
    "            # Extract gender and dominant emotion\n",
    "            gender = parts[2]\n",
    "            emotion = parts[9]\n",
    "\n",
    "            yield gender, emotion\n",
    "\n",
    "    def reducer_count_emotions(self, key, values):\n",
    "        emotion_counts = {}\n",
    "        for emotion in values:\n",
    "            if emotion in emotion_counts:\n",
    "                emotion_counts[emotion] += 1\n",
    "            else:\n",
    "                emotion_counts[emotion] = 1\n",
    "        yield key, emotion_counts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DominantEmotion.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ece3e3-8cb7-4b57-bd87-5ec68718d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver Code \n",
    "import csv\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from dominant_emotion import DominantEmotion\n",
    "\n",
    "def run_mrjob(input_file, output_dir):\n",
    "    # Create a temporary local file to store the intermediate output\n",
    "    temp_output_file = tempfile.NamedTemporaryFile(delete=False, mode='w+')\n",
    "    temp_output_filename = temp_output_file.name\n",
    "\n",
    "    try:\n",
    "        # Run the MRJob with HDFS input and temporary local output\n",
    "        mr_job = DominantEmotion(args=['-r', 'hadoop', input_file, '--output-dir', output_dir])\n",
    "\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "            # Read the intermediate output from HDFS\n",
    "            subprocess.run(['hdfs', 'dfs', '-getmerge', f'{output_dir}/part-*', temp_output_filename], check=True)\n",
    "\n",
    "            # Write the final output to the desired HDFS path\n",
    "            final_output_path = f'{output_dir}/output_emotion.csv'\n",
    "            with open(temp_output_filename, 'r') as infile:\n",
    "                with open('final_output.csv', 'w', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow(['Gender', 'Emotion', 'Count'])\n",
    "\n",
    "                    for line in infile:\n",
    "                        key, value = line.strip().split('\\t')\n",
    "                        gender, emotion_counts = key, eval(value)\n",
    "                        for emotion, count in emotion_counts.items():\n",
    "                            writer.writerow([gender, emotion, count])\n",
    "\n",
    "            # Put the final output CSV to HDFS\n",
    "            subprocess.run(['hdfs', 'dfs', '-put', 'final_output.csv', final_output_path], check=True)\n",
    "\n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        os.remove(temp_output_filename)\n",
    "        if os.path.exists('final_output.csv'):\n",
    "            os.remove('final_output.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'hdfs:///test_data/input.csv'  #  HDFS CSV file\n",
    "    output_dir = 'hdfs:///output_data/'  # HDFS directory\n",
    "    run_mrjob(input_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c0f88-844f-4f6c-8193-5f4732e3aea7",
   "metadata": {},
   "source": [
    "TASK 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7c920-d7f2-4168-ab12-14ffb8c8a02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7784bb-986a-4238-b6aa-0a76307ea8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3c5b1a0-aa4b-4dd1-b716-eb52c22370f8",
   "metadata": {},
   "source": [
    "TASK3 :Average Daily Usage Time by Platform\n",
    "\n",
    "Mapper: Reads each line of the CSV, extracts the platform and daily usage time, and emits a tuple (platform, daily usage time).\n",
    "\n",
    "Reducer: Aggregates the total daily usage time and the count of users per platform, then calculates the average daily usage time for each platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5966d8-2770-4a4d-a13e-16e54a19acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "python\n",
    "Copy code\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class AverageDailyUsageByPlatform(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_usage,\n",
    "                   reducer=self.reducer_calculate_average)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_usage(self, _, line):\n",
    "        if line.startswith('User_ID'):\n",
    "            return\n",
    "\n",
    "        parts = line.split(',')\n",
    "        if len(parts) > 4:\n",
    "            platform = parts[3]\n",
    "            daily_usage = int(parts[4])\n",
    "\n",
    "            yield platform, daily_usage\n",
    "\n",
    "    def reducer_calculate_average(self, key, values):\n",
    "        total_usage = 0\n",
    "        count = 0\n",
    "        for usage in values:\n",
    "            total_usage += usage\n",
    "            count += 1\n",
    "\n",
    "        average_usage = total_usage / count\n",
    "        yield key, average_usage\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    AverageDailyUsageByPlatform.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb869166-bfe4-4089-aeb9-4b0a3d327f28",
   "metadata": {},
   "source": [
    "## Job 4: Total Likes Received by Gender\n",
    "Mapper: Reads each line of the CSV, extracts gender and likes received, and emits a tuple (gender, likes received).\n",
    "\n",
    "Reducer: Aggregates the total likes received for each gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd2a16-89af-4557-8993-948c93b3a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class TotalLikesByGender(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_likes,\n",
    "                   reducer=self.reducer_sum_likes)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_likes(self, _, line):\n",
    "        if line.startswith('User_ID'):\n",
    "            return\n",
    "\n",
    "        parts = line.split(',')\n",
    "        if len(parts) > 6:\n",
    "            gender = parts[2]\n",
    "            likes_received = int(parts[6])\n",
    "\n",
    "            yield gender, likes_received\n",
    "\n",
    "    def reducer_sum_likes(self, key, values):\n",
    "        total_likes = sum(values)\n",
    "        yield key, total_likes\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TotalLikesByGender.run()\n",
    "    \n",
    "Job 5: Average Messages Sent Per Day by Gender\n",
    "Mapper: Reads each line of the CSV, extracts gender and messages sent per day, and emits a tuple (gender, messages sent).\n",
    "\n",
    "Reducer: Aggregates the total messages sent and the count of users per gender, then calculates the average messages sent per day for each gender.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class AverageMessagesByGender(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_messages,\n",
    "                   reducer=self.reducer_calculate_average)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_messages(self, _, line):\n",
    "        if line.startswith('User_ID'):\n",
    "            return\n",
    "\n",
    "        parts = line.split(',')\n",
    "        if len(parts) > 8:\n",
    "            gender = parts[2]\n",
    "            messages_sent = int(parts[8])\n",
    "\n",
    "            yield gender, messages_sent\n",
    "\n",
    "    def reducer_calculate_average(self, key, values):\n",
    "        total_messages = 0\n",
    "        count = 0\n",
    "        for messages in values:\n",
    "            total_messages += messages\n",
    "            count += 1\n",
    "\n",
    "        average_messages = total_messages / count\n",
    "        yield key, average_messages\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    AverageMessagesByGender.run()\n",
    "Job 6: Most Active Users by Platform\n",
    "Mapper: Reads each line of the CSV, extracts the platform and daily usage time, and emits a tuple (platform, (user_id, daily usage time)).\n",
    "\n",
    "Reducer: Aggregates the users per platform and finds the user with the highest daily usage time for each platform.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MostActiveUsersByPlatform(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_usage,\n",
    "                   reducer=self.reducer_find_most_active)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_usage(self, _, line):\n",
    "        if line.startswith('User_ID'):\n",
    "            return\n",
    "\n",
    "        parts = line.split(',')\n",
    "        if len(parts) > 4:\n",
    "            user_id = parts[0]\n",
    "            platform = parts[3]\n",
    "            daily_usage = int(parts[4])\n",
    "\n",
    "            yield platform, (user_id, daily_usage)\n",
    "\n",
    "    def reducer_find_most_active(self, key, values):\n",
    "        most_active_user = None\n",
    "        max_usage = 0\n",
    "        for user_id, daily_usage in values:\n",
    "            if daily_usage > max_usage:\n",
    "                most_active_user = user_id\n",
    "                max_usage = daily_usage\n",
    "\n",
    "        yield key, (most_active_user, max_usage)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MostActiveUsersByPlatform.run()\n",
    "Script to Run Additional MapReduce Jobs\n",
    "python\n",
    "Copy code\n",
    "import csv\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from average_daily_usage_by_platform import AverageDailyUsageByPlatform\n",
    "from total_likes_by_gender import TotalLikesByGender\n",
    "from average_messages_by_gender import AverageMessagesByGender\n",
    "from most_active_users_by_platform import MostActiveUsersByPlatform\n",
    "\n",
    "def run_mrjob(job_class, input_file, output_dir, output_file):\n",
    "    temp_output_file = tempfile.NamedTemporaryFile(delete=False, mode='w+')\n",
    "    temp_output_filename = temp_output_file.name\n",
    "\n",
    "    try:\n",
    "        mr_job = job_class(args=['-r', 'hadoop', input_file, '--output-dir', output_dir])\n",
    "        \n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "            subprocess.run(['hdfs', 'dfs', '-getmerge', f'{output_dir}/part-*', temp_output_filename], check=True)\n",
    "\n",
    "            final_output_path = f'{output_dir}/{output_file}'\n",
    "            with open(temp_output_filename, 'r') as infile:\n",
    "                with open('final_output.csv', 'w', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    \n",
    "                    if job_class == AverageDailyUsageByPlatform:\n",
    "                        writer.writerow(['Platform', 'Average_Daily_Usage'])\n",
    "                        for line in infile:\n",
    "                            key, value = line.strip().split('\\t')\n",
    "                            writer.writerow([key, value])\n",
    "                    elif job_class == TotalLikesByGender:\n",
    "                        writer.writerow(['Gender', 'Total_Likes'])\n",
    "                        for line in infile:\n",
    "                            key, value = line.strip().split('\\t')\n",
    "                            writer.writerow([key, value])\n",
    "                    elif job_class == AverageMessagesByGender:\n",
    "                        writer.writerow(['Gender', 'Average_Messages'])\n",
    "                        for line in infile:\n",
    "                            key, value = line.strip().split('\\t')\n",
    "                            writer.writerow([key, value])\n",
    "                    elif job_class == MostActiveUsersByPlatform:\n",
    "                        writer.writerow(['Platform', 'User_ID', 'Daily_Usage'])\n",
    "                        for line in infile:\n",
    "                            key, value = line.strip().split('\\t')\n",
    "                            user_id, daily_usage = eval(value)\n",
    "                            writer.writerow([key, user_id, daily_usage])\n",
    "\n",
    "            subprocess.run(['hdfs', 'dfs', '-put', 'final_output.csv', final_output_path], check=True)\n",
    "\n",
    "    finally:\n",
    "        os.remove(temp_output_filename)\n",
    "        if os.path.exists('final_output.csv'):\n",
    "            os.remove('final_output.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'hdfs:///test_data/input.csv'\n",
    "    output_dir = 'hdfs:///output_data/'\n",
    "    \n",
    "    run_mrjob(AverageDailyUsageByPlatform, input_file, output_dir, 'output_average_daily_usage.csv')\n",
    "    run_mrjob(TotalLikesByGender, input_file, output_dir, 'output_total_likes.csv')\n",
    "    run_mrjob(AverageMessagesByGender, input_file, output_dir, 'output_average_messages.csv')\n",
    "    run_mrjob(MostActiveUsersByPlatform, input_file, output_dir, 'output_most_active_users.csv')\n",
    "Explanation:\n",
    "AverageDailyUsageByPlatform Class: Calculates the average daily usage time for each platform.\n",
    "TotalLikesByGender Class: Sums up the total likes received by each gender.\n",
    "AverageMessagesByGender Class: Calculates the average messages sent per day for each gender.\n",
    "MostActiveUsersByPlatform Class: Finds the most active user by daily usage time for each platform.\n",
    "run_mrjob Function: Executes the MapReduce job and handles HDFS operations for storing intermediate and final results.\n",
    "Main Script: Calls the run_mrjob function for each of the new classes, processing the CSV file and storing the results in HDFS.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "give me seperate driver python file\n",
    "ChatGPT\n",
    "Certainly! Here are separate Python scripts for driving each of the MapReduce jobs described earlier:\n",
    "\n",
    "Driver Script for Job 3: Average Daily Usage Time by Platform\n",
    "python\n",
    "Copy code\n",
    "from average_daily_usage_by_platform import AverageDailyUsageByPlatform\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'hdfs:///test_data/input.csv'\n",
    "    output_dir = 'hdfs:///output_data/'\n",
    "    output_file = 'output_average_daily_usage.csv'\n",
    "\n",
    "    job = AverageDailyUsageByPlatform(args=['-r', 'hadoop', input_file, '--output-dir', output_dir])\n",
    "    job.run()\n",
    "Driver Script for Job 4: Total Likes Received by Gender\n",
    "python\n",
    "Copy code\n",
    "from total_likes_by_gender import TotalLikesByGender\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'hdfs:///test_data/input.csv'\n",
    "    output_dir = 'hdfs:///output_data/'\n",
    "    output_file = 'output_total_likes.csv'\n",
    "\n",
    "    job = TotalLikesByGender(args=['-r', 'hadoop', input_file, '--output-dir', output_dir])\n",
    "    job.run()\n",
    "Driver Script for Job 5: Average Messages Sent Per Day by Gender\n",
    "python\n",
    "Copy code\n",
    "from average_messages_by_gender import AverageMessagesByGender\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'hdfs:///test_data/input.csv'\n",
    "    output_dir = 'hdfs:///output_data/'\n",
    "    output_file = 'output_average_messages.csv'\n",
    "\n",
    "    job = AverageMessagesByGender(args=['-r', 'hadoop', input_file, '--output-dir', output_dir])\n",
    "    job.run()\n",
    "Driver Script for Job 6: Most Active Users by Platform\n",
    "python\n",
    "Copy code\n",
    "from most_active_users_by_platform import MostActiveUsersByPlatform\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'hdfs:///test_data/input.csv'\n",
    "    output_dir = 'hdfs:///output_data/'\n",
    "    output_file = 'output_most_active_users.csv'\n",
    "\n",
    "    job = MostActiveUsersByPlatform(args=['-r', 'hadoop', input_file, '--output-dir', output_dir])\n",
    "    job.run()\n",
    "You can run each of these scripts independently to execute the corresponding MapReduce job and store the results in HDFS. Make sure to replace the input_file, output_dir, and output_file variables with the appropriate values for your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e018b-d3c8-4558-a7fa-72a12c13999a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed5b541f-9e4b-4794-a1ec-6961c612da2e",
   "metadata": {},
   "source": [
    "**Spark Script for Reading CSV Data**\n",
    "This script demonstrates how to read a CSV file into a Spark DataFrame and explore its contents using SparkSession.\n",
    "\n",
    "1. Spark Session Initialization:\n",
    "\n",
    "Imports the SparkSession class from pyspark.sql.\n",
    "Creates a new SparkSession using the builder method.\n",
    "Sets the application name to \"Read CSV into DataFrame\" for identification in the Spark UI and logs (optional).\n",
    "Calls getOrCreate() to create a new SparkSession if it doesn't exist, or retrieve the existing one if it does.\n",
    "Stores the SparkSession in the variable spark.\n",
    "2.  Reading CSV File:\n",
    "\n",
    "Uses the spark.read.csv method to read the CSV file named \"test.csv\" into a DataFrame.\n",
    "Sets the header=True option (optional but recommended) to indicate that the CSV file has a header row containing column names.\n",
    "3. Exploring the DataFrame:\n",
    "\n",
    "Calls df.printSchema() to print the DataFrame schema, which shows the names and data types of each column.\n",
    "Calls df.show() to display the first few rows of the DataFrame, providing a glimpse of the data content.\n",
    "Running the Script:\n",
    "\n",
    "Save the script as a Python file (e.g., read_csv.py).\n",
    "Ensure you have a Spark environment set up and the pyspark library installed.\n",
    "Run the script from the command line using spark-submit read_csv.py.\n",
    "This script provides a basic example of how to interact with CSV data using Spark. You can further manipulate and analyze the DataFrame using Spark SQL operations or other Spark functionalities depending on your specific needs.\n",
    "\n",
    "Additional Notes:\n",
    "\n",
    "The script assumes the CSV file \"test.csv\" is located in the same directory as the script itself.\n",
    "You can modify the script to:\n",
    "Read a CSV file from a different location by specifying the full path.\n",
    "Read a CSV file with a different delimiter (e.g., tab-separated) by using the delimiter option in spark.read.csv.\n",
    "Read a specific number of rows using the limit option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33908c2f-cb3e-4368-b228-25d4d4a9ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009e5f6-767c-41f1-9669-541f39a532d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV into DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "df = spark.read.csv(\"test.csv\", header=True)\n",
    "\n",
    "# Show the DataFrame schema and first few rows\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628c65e-1e39-444a-a949-55152bb2009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858d4f8-f6c5-4e44-be7b-f77c1e83786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c8a45d-e11c-4cba-a2cb-bb2f68b0c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import upper\n",
    "\n",
    "type(df.Gender) == type(upper(df.Gender)) == type(df.Gender.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be48c4d-7527-4f78-bfb2-fbcecdcc0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CSV to DataFrame\").getOrCreate()\n",
    "\n",
    "# Define the schema for your DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"User_ID\", IntegerType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Platform\", StringType(), True),\n",
    "    StructField(\"Daily_Usage_Time (minutes)\", IntegerType(), True),\n",
    "    StructField(\"Posts_Per_Day\", IntegerType(), True),\n",
    "    StructField(\"Likes_Received_Per_Day\", IntegerType(), True),\n",
    "    StructField(\"Comments_Received_Per_Day\", IntegerType(), True),\n",
    "    StructField(\"Messages_Sent_Per_Day\", IntegerType(), True),\n",
    "    StructField(\"Dominant_Emotion\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"test.csv\", header=False, schema=schema)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop the SparkSession if you created one in this script\n",
    "#spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab21fea-237e-4c9d-8e8c-9322e15e250f",
   "metadata": {},
   "source": [
    "## Read test.csv in pandaDtaframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3a995-c618-4324-985a-c4315f3e073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, date\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CSV to Spark DataFrame\").getOrCreate()\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'test.csv'\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame\n",
    "pandas_df = pd.read_csv(csv_file_path, header=None, names=[\n",
    "    \"User_ID\", \"Age\", \"Gender\", \"Platform\", \"Daily_Usage_Time (minutes)\",\n",
    "    \"Posts_Per_Day\", \"Likes_Received_Per_Day\", \"Comments_Received_Per_Day\",\n",
    "    \"Messages_Sent_Per_Day\", \"Dominant_Emotion\"\n",
    "])\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spark DataFrame\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# Display the Spark DataFrame\n",
    "spark_df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "#spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe94d4-3351-44b0-beef-26d337882885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
